{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2d4b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dfg import check_dfg_compliance\n",
    "\n",
    "def model_eval_test(dataset_name, csvwriter=None):\n",
    "    '''\n",
    "       This module is for validation and testing the Generator\n",
    "       @param modelG: Generator neural network\n",
    "       @param mode: 'validation', 'test', 'test-validation'\n",
    "       @param obj: A data object created from \"Input\" class that contains the required information\n",
    "       @return: The accuracy of the Generator\n",
    "       '''\n",
    "    # set the evaluation mode (this mode is necessary if you train with batch, since in test the size of batch is different)\n",
    "    \n",
    "    batch = 1\n",
    "    #events = list(np.arange(0, len(obj.unique_event) + 1))\n",
    "    predicted = []\n",
    "    accuracy = []\n",
    "    mae = []\n",
    "    dfg_compliance_pred = []\n",
    "    dfg_compliance_gt = []\n",
    "    y_truth_list = []\n",
    "    y_pred_last_event_list = []\n",
    "    accuracy_last1 = []\n",
    "    accuracy_last2 = []\n",
    "    accuracy_last3 = []\n",
    "    lastk = {}\n",
    "    df = pd.read_pickle(\"dataset/preprocessed/\"+dataset_name+\"_design_mat.pkl\")\n",
    "    unique_event = [0] + sorted(df['class'].unique())\n",
    "    events = list(np.arange(0, len(unique_event)))\n",
    "    print(\"events\",events)\n",
    "    max_activities = len(unique_event)\n",
    "    dur = 0\n",
    "    dur_gt = 0\n",
    "    num_overall_goal_satisfied = 0\n",
    "    num_overall_goal_not_satisfied = 0\n",
    "    num_overall_goal_satisfied_gt = 0\n",
    "    num_overall_goal_not_satisfied_gt = 0\n",
    "    row = []\n",
    "    selected_columns = np.arange(0,max_activities+1)\n",
    "    y_pred_last_event_list_prev = None\n",
    "    group = df.groupby('CaseID')\n",
    "    cur_prefix_len = 1\n",
    "    prev_event = None\n",
    "    max_prefix_len = 13\n",
    "    pred_events = []\n",
    "    thresh = 13.89\n",
    "    if dataset_name == \"helpdesk\":\n",
    "        thresh = 13.89 \n",
    "        max_prefix_len = 13\n",
    "    if dataset_name == \"bpi_12_w\":\n",
    "        thresh = 18.28\n",
    "        max_prefix_len = 73\n",
    "    if dataset_name == \"traffic_ss\":\n",
    "        thresh = 607.04\n",
    "        max_prefix_len = 16\n",
    "\n",
    "    for name,gr in group:\n",
    "        gr = gr.reset_index(drop=True)\n",
    "        new_row = [0] * gr.shape[1]\n",
    "        gr.loc[gr.shape[0]] = new_row\n",
    "        gr.iloc[gr.shape[0] - 1, gr.columns.get_loc('0')] = 1  # End of line is denoted by class 0\n",
    "        temp = torch.tensor(gr.values, dtype=torch.float, requires_grad=False)\n",
    "        temp_shifted = torch.tensor(gr[['duration_time','class']].values, dtype=torch.float, requires_grad=False)\n",
    "        x = pad_sequence([temp], batch_first=True)\n",
    "        y_truth = pad_sequence([temp_shifted], batch_first=True)\n",
    "        cur_trace_len = int(x.shape[1])\n",
    "        if cur_trace_len > max_prefix_len:\n",
    "            continue\n",
    "        activites = torch.argmax(x[:, :, events])\n",
    "        # When we create mini batches, the length of the last one probably is less than the batch size, and it makes problem for the LSTM, therefore we skip it.\n",
    "        if (x.size()[0] < batch):\n",
    "            continue\n",
    "        for cur_prefix_len in range(1,cur_trace_len):\n",
    "            \n",
    "            # Separating event and timestamp\n",
    "            y_truth_timestamp = y_truth[:, cur_prefix_len, 0].view(batch, 1, -1)\n",
    "            \n",
    "            y_truth_event = y_truth[:, cur_prefix_len, 1].view(batch, 1, -1)\n",
    "            \n",
    "            # Executing LSTM\n",
    "            if cur_prefix_len == 1:\n",
    "                x_inn = x[:,  :cur_prefix_len, selected_columns]\n",
    "                prev_event = x[:,  :cur_prefix_len, len(selected_columns)+1].ravel().detach().numpy().astype(\"int\")\n",
    "            else:\n",
    "                x_inn = x[:,  :cur_prefix_len, selected_columns]\n",
    "                \n",
    "            rnnG = torch.load(\"checkpoints/\"+dataset_name+\"/event_timestamp_prediction/prefix_\"+str(cur_prefix_len)+\"/rnnG.m\")\n",
    "            rnnG.eval()\n",
    "            y_pred = rnnG(x_inn[:,  :cur_prefix_len, selected_columns])\n",
    "\n",
    "            # Just taking the last predicted element from each the batch\n",
    "            y_pred_last = y_pred[0: batch, cur_prefix_len - 1, :]\n",
    "            y_pred_last = y_pred_last.view((batch, 1, -1))\n",
    "            y_pred_last_event = torch.argmax(F.softmax(y_pred_last[:, :, events], dim=2), dim=2)\n",
    "\n",
    "            #Storing list of predictions and corresponding ground truths (to be used for f1score)\n",
    "            y_truth_list += list(y_truth_event.flatten().data.cpu().numpy().astype(int))\n",
    "            y_pred_last_event_list += list(y_pred_last_event.flatten().data.cpu().numpy().astype(int))\n",
    "            \n",
    "            # checking dfg compliance for predicted event\n",
    "            if not(int(prev_event[0]) == 0 and int(y_pred_last_event.detach()[0][0])==0):\n",
    "                dfg_compliance_bool = check_dfg_compliance(prev_event , y_pred_last_event.detach(), dset=dataset_name)\n",
    "                dfg_compliance_pred.append(int(dfg_compliance_bool))\n",
    "\n",
    "                dfg_compliance_gt_bool = check_dfg_compliance(prev_event , y_truth_event.detach().reshape(y_pred_last_event.shape), dset=dataset_name)\n",
    "                dfg_compliance_gt.append(int(dfg_compliance_gt_bool))\n",
    "\n",
    "                if y_pred_last_event.flatten().data.cpu().numpy().astype(int)==y_truth_event.flatten().data.cpu().numpy().astype(int):\n",
    "                    accuracy.append(1)\n",
    "                else:\n",
    "                    accuracy.append(0)\n",
    "                pred_events.append(int(y_pred_last_event.detach()[0][0]))\n",
    "\n",
    "            # Computing MAE for the timestamp\n",
    "            y_pred_timestamp = y_pred_last[:, :, len(events)].view((batch, 1, -1))\n",
    "            mae.append(torch.abs(y_truth_timestamp - y_pred_timestamp).mean().detach())\n",
    "            dur += max(y_pred_timestamp.detach().numpy()[0][0],0)  #adding to total proccess duration, making sure y_pred is non-negative\n",
    "            dur_gt += max(y_truth_timestamp.detach().numpy()[0][0],0)\n",
    "            prev_event = y_pred_last_event.ravel().detach().numpy().astype(\"int\")\n",
    "            \n",
    "            if prev_event ==0:  #model predicted 0\n",
    "                break\n",
    "            \n",
    "        # GS cases\n",
    "        try:\n",
    "            if dur < thresh:\n",
    "                num_overall_goal_satisfied += 1\n",
    "            else:\n",
    "                num_overall_goal_not_satisfied += 1\n",
    "\n",
    "            if dur_gt < thresh:\n",
    "                num_overall_goal_satisfied_gt += 1\n",
    "            else:\n",
    "                num_overall_goal_not_satisfied_gt += 1\n",
    "        except Exception as e: print(e)\n",
    "#         print(pred_events)  \n",
    "        lenn = min(5, len(pred_events))\n",
    "        if lenn<=2:\n",
    "            lenn = 3\n",
    "        if len(pred_events)>=2:\n",
    "            for i in range(2,lenn):\n",
    "                if pred_events[-i] in lastk:\n",
    "                    lastk[pred_events[-i]] += 1\n",
    "                else:\n",
    "                    lastk[pred_events[-i]] = 1\n",
    "                if i == 1+1:\n",
    "                    if(pred_events[-i] == y_truth_list[-i]): #action chosen by RL agent = gt\n",
    "                        accuracy_last1.append(1)\n",
    "                        accuracy_last2.append(1)\n",
    "                        accuracy_last3.append(1)\n",
    "                    else:\n",
    "                        accuracy_last1.append(0)\n",
    "                        accuracy_last2.append(0)\n",
    "                        accuracy_last3.append(0)\n",
    "                if i == 2+1:\n",
    "                    if(pred_events[-i] == y_truth_list[-i]): #action chosen by RL agent = gt\n",
    "                        accuracy_last2.append(1)\n",
    "                        accuracy_last3.append(1)\n",
    "                    else:\n",
    "                        accuracy_last2.append(0)\n",
    "                        accuracy_last3.append(0)\n",
    "                if i == 3+1:\n",
    "                    if(pred_events[-i] == y_truth_list[-i]): #action chosen by RL agent = gt\n",
    "                        accuracy_last3.append(1)\n",
    "                    else:\n",
    "                        accuracy_last3.append(0)\n",
    "        dur = 0\n",
    "        dur_gt = 0\n",
    "        pred_events = []\n",
    "    tot = num_overall_goal_satisfied+ num_overall_goal_not_satisfied\n",
    "    gs = num_overall_goal_satisfied/tot\n",
    "    gv = num_overall_goal_not_satisfied/tot\n",
    "    \n",
    "    tot_gt = num_overall_goal_satisfied_gt+ num_overall_goal_not_satisfied_gt\n",
    "    gs_gt = num_overall_goal_satisfied_gt/tot_gt\n",
    "    gv_gt = num_overall_goal_not_satisfied_gt/tot_gt\n",
    "    print(\"lastk\",lastk)\n",
    "    print(\"compliance pred\", np.mean(np.array(dfg_compliance_pred)))\n",
    "    print(\"compliance gt\", np.mean(np.array(dfg_compliance_gt)))\n",
    "    print(\"gs \", gs)\n",
    "    print(\"gv \", gv)\n",
    "    print(\"gs gt \", gs_gt)\n",
    "    print(\"gv gt \", gv_gt)\n",
    "    print(\"acc \",np.mean(np.array(accuracy)))\n",
    "    print(\"acc last 3\",np.mean(np.array(accuracy_last3)))\n",
    "    print(\"acc last 2\",np.mean(np.array(accuracy_last2)))\n",
    "    print(\"acc last 1\",np.mean(np.array(accuracy_last1)))\n",
    "    print(\"mae \", np.mean(np.array(mae)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ab2f129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:  helpdesk\n",
      "events [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "lastk {6: 5153, 8: 249}\n",
      "compliance pred 0.8178766588602654\n",
      "compliance gt 0.7641686182669789\n",
      "gs  0.8550762756443977\n",
      "gv  0.1449237243556023\n",
      "gs gt  0.7498684902682798\n",
      "gv gt  0.25013150973172016\n",
      "acc  0.7395784543325526\n",
      "acc last 3 0.6260644205849686\n",
      "acc last 2 0.6821486268174475\n",
      "acc last 1 0.8852631578947369\n",
      "mae  3.3614337\n",
      "dataset:  bpi_12_w\n",
      "events [0, 1, 2, 3, 4, 5, 6]\n",
      "lastk {5: 8661, 6: 621, 3: 5490, 4: 3670, 1: 264}\n",
      "compliance pred 0.9331456938280006\n",
      "compliance gt 0.8654717535530365\n",
      "gs  0.7924821373097235\n",
      "gv  0.20751786269027647\n",
      "gs gt  0.7630734182458321\n",
      "gv gt  0.23692658175416795\n",
      "acc  0.6742031764687688\n",
      "acc last 3 0.7146370148615417\n",
      "acc last 2 0.6664921857474014\n",
      "acc last 1 0.5734360221591677\n",
      "mae  1.7013488\n",
      "dataset:  traffic_ss\n",
      "events [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "lastk {5: 7291, 4: 8613, 3: 7064, 2: 6634, 6: 951, 8: 348, 9: 98, 10: 76}\n",
      "compliance pred 0.8214648399223218\n",
      "compliance gt 0.91706604429083\n",
      "gs  0.502760226139009\n",
      "gv  0.497239773860991\n",
      "gs gt  0.7509810442301297\n",
      "gv gt  0.2490189557698703\n",
      "acc  0.6482923266047854\n",
      "acc last 3 0.7567497988736926\n",
      "acc last 2 0.7019155759729566\n",
      "acc last 1 0.583372131692717\n",
      "mae  68.17868\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"helpdesk\", \"bpi_12_w\",\"traffic_ss\"]  \n",
    "for dataset in datasets:\n",
    "    print(\"dataset: \",dataset)\n",
    "    model_eval_test(dataset_name=dataset, csvwriter=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0e426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# from dfg import check_dfg_compliance\n",
    "\n",
    "# def model_eval_test(dataset_name, csvwriter=None):\n",
    "#     '''\n",
    "#        This module is for validation and testing the Generator\n",
    "#        @param modelG: Generator neural network\n",
    "#        @param mode: 'validation', 'test', 'test-validation'\n",
    "#        @param obj: A data object created from \"Input\" class that contains the required information\n",
    "#        @return: The accuracy of the Generator\n",
    "#        '''\n",
    "#     # set the evaluation mode (this mode is necessary if you train with batch, since in test the size of batch is different)\n",
    "    \n",
    "#     batch = 1\n",
    "#     #events = list(np.arange(0, len(obj.unique_event) + 1))\n",
    "#     predicted = []\n",
    "#     accuracy = []\n",
    "#     mae = []\n",
    "#     dfg_compliance_pred = []\n",
    "#     dfg_compliance_gt = []\n",
    "#     y_truth_list = []\n",
    "#     y_pred_last_event_list = []\n",
    "#     df = pd.read_pickle(\"dataset/preprocessed/\"+dataset_name+\"_design_mat.pkl\")\n",
    "#     unique_event = [0] + sorted(df['class'].unique())\n",
    "#     events = list(np.arange(0, len(unique_event)))\n",
    "#     print(\"events\",events)\n",
    "#     max_activities = len(unique_event)\n",
    "#     dur = 0\n",
    "#     dur_gt = 0\n",
    "#     num_overall_goal_satisfied = 0\n",
    "#     num_overall_goal_not_satisfied = 0\n",
    "#     num_overall_goal_satisfied_gt = 0\n",
    "#     num_overall_goal_not_satisfied_gt = 0\n",
    "#     row = []\n",
    "#     selected_columns = np.arange(0,max_activities+1)\n",
    "#     y_pred_last_event_list_prev = None\n",
    "#     group = df.groupby('CaseID')\n",
    "#     cur_prefix_len = 1\n",
    "#     prev_event = None\n",
    "#     max_prefix_len = 13\n",
    "#     thresh = 13.89\n",
    "#     if dataset_name == \"helpdesk\":\n",
    "#         thresh = 13.89 \n",
    "#         max_prefix_len = 13\n",
    "#     if dataset_name == \"bpi_12_w\":\n",
    "#         thresh = 18.28\n",
    "#         max_prefix_len = 73\n",
    "#     if dataset_name == \"traffic_ss\":\n",
    "#         thresh = 607.04\n",
    "#         max_prefix_len = 16\n",
    "\n",
    "#     for name,gr in group:\n",
    "#         gr = gr.reset_index(drop=True)\n",
    "#         new_row = [0] * gr.shape[1]\n",
    "#         gr.loc[gr.shape[0]] = new_row\n",
    "#         gr.iloc[gr.shape[0] - 1, gr.columns.get_loc('0')] = 1  # End of line is denoted by class 0\n",
    "#         temp = torch.tensor(gr.values, dtype=torch.float, requires_grad=False)\n",
    "#         temp_shifted = torch.tensor(gr[['duration_time','class']].values, dtype=torch.float, requires_grad=False)\n",
    "#         x = pad_sequence([temp], batch_first=True)\n",
    "#         y_truth = pad_sequence([temp_shifted], batch_first=True)\n",
    "#         cur_trace_len = int(x.shape[1])\n",
    "#         if cur_trace_len > max_prefix_len:\n",
    "#             continue\n",
    "#         activites = torch.argmax(x[:, :, events])\n",
    "#         # When we create mini batches, the length of the last one probably is less than the batch size, and it makes problem for the LSTM, therefore we skip it.\n",
    "#         if (x.size()[0] < batch):\n",
    "#             continue\n",
    "#         for cur_prefix_len in range(1,cur_trace_len):\n",
    "            \n",
    "#             # Separating event and timestamp\n",
    "#             y_truth_timestamp = y_truth[:, cur_prefix_len, 0].view(batch, 1, -1)\n",
    "            \n",
    "#             y_truth_event = y_truth[:, cur_prefix_len, 1].view(batch, 1, -1)\n",
    "            \n",
    "#             # Executing LSTM\n",
    "#             if cur_prefix_len == 1:\n",
    "#                 x_inn = x[:,  :cur_prefix_len, selected_columns]\n",
    "#                 prev_event = x[:,  :cur_prefix_len, len(selected_columns)+1].ravel().detach().numpy().astype(\"int\")\n",
    "#             else:\n",
    "#                 x_inn = x[:,  :cur_prefix_len, selected_columns]\n",
    "                \n",
    "#             rnnG = torch.load(\"checkpoints/\"+dataset_name+\"/event_timestamp_prediction/prefix_\"+str(cur_prefix_len)+\"/rnnG.m\")\n",
    "#             rnnG.eval()\n",
    "#             y_pred = rnnG(x_inn[:,  :cur_prefix_len, selected_columns])\n",
    "\n",
    "#             # Just taking the last predicted element from each the batch\n",
    "#             y_pred_last = y_pred[0: batch, cur_prefix_len - 1, :]\n",
    "#             y_pred_last = y_pred_last.view((batch, 1, -1))\n",
    "#             y_pred_last_event = torch.argmax(F.softmax(y_pred_last[:, :, events], dim=2), dim=2)\n",
    "\n",
    "#             #Storing list of predictions and corresponding ground truths (to be used for f1score)\n",
    "#             y_truth_list += list(y_truth_event.flatten().data.cpu().numpy().astype(int))\n",
    "#             y_pred_last_event_list += list(y_pred_last_event.flatten().data.cpu().numpy().astype(int))\n",
    "\n",
    "#             # checking dfg compliance for predicted event\n",
    "#             if not(int(prev_event[0]) == 0 and int(y_pred_last_event.detach()[0][0])==0):\n",
    "#                 dfg_compliance_bool = check_dfg_compliance(prev_event , y_pred_last_event.detach(), dset=dataset_name)\n",
    "#                 dfg_compliance_pred.append(int(dfg_compliance_bool))\n",
    "\n",
    "#                 dfg_compliance_gt_bool = check_dfg_compliance(prev_event , y_truth_event.detach().reshape(y_pred_last_event.shape), dset=dataset_name)\n",
    "#                 dfg_compliance_gt.append(int(dfg_compliance_gt_bool))\n",
    "\n",
    "#                 if y_pred_last_event.flatten().data.cpu().numpy().astype(int)==y_truth_event.flatten().data.cpu().numpy().astype(int):\n",
    "#                     accuracy.append(1)\n",
    "#                 else:\n",
    "#                     accuracy.append(0)\n",
    "\n",
    "#             # Computing MAE for the timestamp\n",
    "#             y_pred_timestamp = y_pred_last[:, :, len(events)].view((batch, 1, -1))\n",
    "#             mae.append(torch.abs(y_truth_timestamp - y_pred_timestamp).mean().detach())\n",
    "#             dur += max(y_pred_timestamp.detach().numpy()[0][0],0)  #adding to total proccess duration, making sure y_pred is non-negative\n",
    "#             dur_gt += max(y_truth_timestamp.detach().numpy()[0][0],0)\n",
    "#             prev_event = y_pred_last_event.ravel().detach().numpy().astype(\"int\")\n",
    "            \n",
    "#         # GS cases\n",
    "#         try:\n",
    "#             if dur < thresh:\n",
    "#                 num_overall_goal_satisfied += 1\n",
    "#             else:\n",
    "#                 num_overall_goal_not_satisfied += 1\n",
    "\n",
    "#             if dur_gt < thresh:\n",
    "#                 num_overall_goal_satisfied_gt += 1\n",
    "#             else:\n",
    "#                 num_overall_goal_not_satisfied_gt += 1\n",
    "#         except Exception as e: print(e)\n",
    "#         dur = 0\n",
    "#         dur_gt = 0\n",
    "#     tot = num_overall_goal_satisfied+ num_overall_goal_not_satisfied\n",
    "#     gs = num_overall_goal_satisfied/tot\n",
    "#     gv = num_overall_goal_not_satisfied/tot\n",
    "    \n",
    "#     tot_gt = num_overall_goal_satisfied_gt+ num_overall_goal_not_satisfied_gt\n",
    "#     gs_gt = num_overall_goal_satisfied_gt/tot_gt\n",
    "#     gv_gt = num_overall_goal_not_satisfied_gt/tot_gt\n",
    "#     print(\"compliance pred\", np.mean(np.array(dfg_compliance_pred)))\n",
    "#     print(\"compliance gt\", np.mean(np.array(dfg_compliance_gt)))\n",
    "#     print(\"gs \", gs)\n",
    "#     print(\"gv \", gv)\n",
    "#     print(\"gs gt\", gs_gt)\n",
    "#     print(\"gv gt\", gv_gt)\n",
    "#     print(\"acc \",np.mean(np.array(accuracy)))\n",
    "#     print(\"mae \", np.mean(np.array(mae)))\n",
    "# datasets = [\"traffic_ss\"]  \n",
    "\n",
    "# for dataset in datasets:\n",
    "#     print(\"dataset: \",dataset)\n",
    "#     model_eval_test(dataset_name=dataset, csvwriter=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
